{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import config\n",
    "v1, v2, v3 = config.spark_env(\"msi\")\n",
    "os.environ['SPARK_VERSION'] = v1\n",
    "os.environ['JAVA_HOME'] = v2\n",
    "os.environ['SPARK_HOME'] = v3\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.appName(\"emissionsdataframe\").getOrCreate()\n",
    "from sqlalchemy import create_engine, insert\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import MetaData, update, Table\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import array, col, explode, lit, struct\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_string = config.rds_string\n",
    "engine = create_engine(f'postgresql://{rds_string}')\n",
    "conn = engine.connect()\n",
    "metadata = MetaData(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_spark(table):\n",
    "    from pyspark.sql import Row\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy import MetaData, Table\n",
    "    metadata = MetaData(engine)\n",
    "    table_var = Table(table, metadata, autoload=True, autoload_with=engine)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=engine)\n",
    "    session = Session()\n",
    "    query = session.query(table_var).all()\n",
    "    steve = table_var.metadata.tables[table].columns.keys()\n",
    "    query_list = []\n",
    "    for i in query:\n",
    "        q_len = len(i)\n",
    "        temp_dict = {}\n",
    "        for j in range(q_len):\n",
    "            key = steve[j]\n",
    "            value = i[j]\n",
    "            if value == None:\n",
    "                value = float(0)\n",
    "            temp_dict[key] = value\n",
    "            if j == (q_len - 1):\n",
    "                query_list.append(temp_dict)\n",
    "    df = spark.createDataFrame(Row(**x) for x in query_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_spark(table):\n",
    "    from pyspark.sql import Row\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy import MetaData, Table\n",
    "    metadata = MetaData(engine)\n",
    "    table_var = Table(table, metadata, autoload=True, autoload_with=engine)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=engine)\n",
    "    session = Session()\n",
    "    query = session.query(table_var).all()\n",
    "    steve = table_var.metadata.tables[table].columns.keys()\n",
    "    query_list = []\n",
    "    for i in query:\n",
    "        q_len = len(i)\n",
    "        temp_dict = {}\n",
    "        for j in range(q_len):\n",
    "            key = steve[j]\n",
    "            value = i[j]\n",
    "            if value == None:\n",
    "                value = float(0)\n",
    "            temp_dict[key] = value\n",
    "            if j == (q_len - 1):\n",
    "                query_list.append(temp_dict)\n",
    "    df = spark.createDataFrame(Row(**x) for x in query_list)\n",
    "    return df\n",
    "\n",
    "def query_maker(table):\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy import MetaData, Table\n",
    "    metadata = MetaData(engine)\n",
    "    table_var = Table(table, metadata, autoload=True, autoload_with=engine)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=engine)\n",
    "    session = Session()\n",
    "    query = session.query(table_var).all()\n",
    "    steve = table_var.metadata.tables[table].columns.keys()\n",
    "    return (query, steve)\n",
    "\n",
    "def spark_maker(query,country):\n",
    "    c_list = []\n",
    "    for i in query:\n",
    "        q_len = len(i)\n",
    "        if i[0] == country:\n",
    "            temp_dict = {}\n",
    "            for j in range(q_len):\n",
    "                key = steve[j]\n",
    "                value = i[j]\n",
    "                if value == None:\n",
    "                    value = float(0)\n",
    "                temp_dict[key] = value\n",
    "                if j == (q_len - 1):\n",
    "                    c_list.append(temp_dict)\n",
    "    df = spark.createDataFrame(Row(**x) for x in c_list) \n",
    "    return df\n",
    "\n",
    "def sql_spark_trade(table, country):\n",
    "    from sqlalchemy.orm import Session, sessionmaker\n",
    "    from sqlalchemy import MetaData, Table\n",
    "    metadata = MetaData(engine)\n",
    "    table_var = Table(table, metadata, autoload=True, autoload_with=engine)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=engine)\n",
    "    session = Session()\n",
    "    query = session.query(table_var).filter(table_var.c.country_or_area == country).all()\n",
    "    steve = table_var.metadata.tables[table].columns.keys()\n",
    "    c_list = []\n",
    "    for i in query:\n",
    "        q_len = len(i)\n",
    "        if i[0] == country:\n",
    "            temp_dict = {}\n",
    "            for j in range(q_len):\n",
    "                key = steve[j]\n",
    "                value = i[j]\n",
    "                if value == None:\n",
    "                    value = float(0)\n",
    "                temp_dict[key] = value\n",
    "                if j == (q_len - 1):\n",
    "                    c_list.append(temp_dict)\n",
    "    df = spark.createDataFrame(Row(**x) for x in c_list)\n",
    "    df = df.withColumn(\"year\",df.year.cast('int'))\n",
    "    df = df.withColumnRenamed(\"country_or_area\",\"country\")\n",
    "    df_export = df.filter(df['flow'] == \"Export\")\n",
    "    df_import = df.filter(df['flow'] == \"Import\")\n",
    "    return (df_export, df_import)\n",
    "\n",
    "def melt(\n",
    "        df: DataFrame, \n",
    "        id_vars: Iterable[str], value_vars: Iterable[str], \n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = array(*(\n",
    "        struct(lit(c).alias(var_name), col(c).alias(value_name)) \n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)\n",
    "\n",
    "def min_max(df,column):\n",
    "    max_ = df.agg({column: 'max'})\n",
    "    min_ = df.agg({column: 'min'})\n",
    "    return (min_, max_)\n",
    "\n",
    "def min_int(df,column):\n",
    "    arr = np.array(df.select(column).collect())\n",
    "    y_l = []\n",
    "    for i in arr:\n",
    "        y_l.append(int(i[0]))\n",
    "    min_ = min(y_l)\n",
    "    max_ = max(y_l)\n",
    "    return (min_, max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData, Table\n",
    "metadata = MetaData(engine)\n",
    "global_trade = Table(\"global_trade\", metadata, autoload=True, autoload_with=engine)\n",
    "trade_schema = global_trade.metadata.tables[\"global_trade\"].columns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_india=sql_to_spark(\"india_export_data\")\n",
    "spark_emissions=sql_to_spark('global_emissions')\n",
    "spark_emissions = spark_emissions.withColumnRenamed(\"Entity\",\"country\")\n",
    "spark_emissions = spark_emissions.withColumnRenamed(\"Year\",\"year\")\n",
    "spark_gdp=sql_to_spark('gdp_data')\n",
    "gdp_col = spark_gdp.columns\n",
    "gdp_col2 = gdp_col[2:]\n",
    "gdp_static_cols = gdp_col[:2]\n",
    "spark_gdp_2 = melt(spark_gdp, id_vars=gdp_static_cols, value_vars=gdp_col2)\n",
    "spark_gdp_2 = spark_gdp_2.withColumn(\"variable\",spark_gdp_2.variable.cast('int'))\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"variable\",\"year\")\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"value\",\"GDP\")\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"Country \",\"country\")\n",
    "spark_gdp_2 = spark_gdp_2.select(['country','year','GDP'])\n",
    "india_export_df, india_import_df = sql_spark_trade(\"global_trade\",\"India\")\n",
    "india_export_df_min, india_export_df_max = min_int(india_export_df, 'year')\n",
    "india_import_df_min, india_import_df_max = min_int(india_import_df, 'year')\n",
    "india_gdp_spark = spark_gdp_2.filter(spark_gdp_2[\"country\"] == \"India\")\n",
    "india_gdp_spark_min, india_gdp_spark_max = min_int(india_gdp_spark, \"year\")\n",
    "india_emissions = spark_emissions.filter(spark_emissions[\"country\"] == \"India\")\n",
    "india_emissions_min, india_emissions_max = min_int(india_emissions, 'year')\n",
    "years_df = pd.DataFrame([{\"min\":india_export_df_min, \"max\":india_export_df_max},\n",
    " {\"min\":india_import_df_min, \"max\":india_import_df_max},\n",
    " {\"min\":india_gdp_spark_min, \"max\":india_gdp_spark_max},\n",
    " {\"min\":india_emissions_min, \"max\":india_emissions_max},\n",
    " {\"min\":spark_india_min,\"max\":spark_india_max}])\n",
    "lower_bound = years_df['min'].max()\n",
    "upper_bound = years_df['max'].min()\n",
    "years_df = pd.DataFrame([{\"min\":india_export_df_min, \"max\":india_export_df_max},\n",
    " {\"min\":india_import_df_min, \"max\":india_import_df_max},\n",
    " {\"min\":india_gdp_spark_min, \"max\":india_gdp_spark_max},\n",
    " {\"min\":india_emissions_min, \"max\":india_emissions_max},\n",
    "#  {\"min\":spark_india_min,\"max\":spark_india_max}])\n",
    "                        ])\n",
    "lower_bound = years_df['min'].max()\n",
    "upper_bound = years_df['max'].min()\n",
    "india_gdp_filt = india_gdp_spark.filter(india_gdp_spark[\"year\"] >= lower_bound)\n",
    "india_gdp_filt = india_gdp_filt.filter(india_gdp_filt[\"year\"] <= upper_bound)\n",
    "india_emissions_filt = india_emissions.filter(india_emissions[\"year\"] >= lower_bound)\n",
    "india_emissions_filt = india_emissions_filt.filter(india_emissions_filt[\"year\"] <= upper_bound)\n",
    "india_import_filt = india_import_df.filter(india_import_df[\"year\"] >= lower_bound)\n",
    "india_import_filt = india_import_filt.filter(india_import_filt[\"year\"] <= upper_bound)\n",
    "india_export_filt = india_export_df.filter(india_export_df[\"year\"] >= lower_bound)\n",
    "india_export_filt = india_export_filt.filter(india_export_filt[\"year\"] <= upper_bound)\n",
    "merged_df = india_gdp_filt.join(india_emissions_filt, india_gdp_filt.year == india_emissions_filt.year, 'outer') \\\n",
    ".select(india_gdp_filt.country ,india_gdp_filt.year,india_gdp_filt.GDP, india_emissions_filt.annual_co2_emissions_tonnes) \\\n",
    ".distinct()\n",
    "merged_df = merged_df.orderBy(merged_df.year.asc())\n",
    "india_import_sum = india_import_filt.groupBy('year').sum()\n",
    "india_import_sum = india_import_sum.orderBy(india_import_sum.year.asc())\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(trade_usd)\",\"import_trade_sum_usd\")\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(weight_kg)\",\"import_weight_sum_kg\")\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(quantity)\",\"import_quantity_sum\")\n",
    "india_export_sum = india_export_filt.groupBy('year').sum()\n",
    "india_export_sum = india_export_sum.orderBy(india_export_sum.year.asc())\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(trade_usd)\",\"export_trade_sum_usd\")\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(weight_kg)\",\"export_weight_sum_kg\")\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(quantity)\",\"export_quantity_sum\")\n",
    "merged_df2 = merged_df.join(india_import_sum, merged_df.year == india_import_sum.year, 'outer') \\\n",
    ".select(merged_df.country ,merged_df.year,merged_df.GDP, merged_df.annual_co2_emissions_tonnes,\n",
    "       india_import_sum['import_trade_sum_usd'],india_import_sum['import_weight_sum_kg'],india_import_sum['import_quantity_sum']) \\\n",
    ".distinct()\n",
    "merged_df2 = merged_df2.orderBy(merged_df2.year.asc())\n",
    "final_merged_spark = merged_df2.join(india_export_sum, merged_df2.year == india_export_sum.year, 'outer') \\\n",
    ".select(merged_df2.country ,merged_df2.year,merged_df2.GDP, merged_df.annual_co2_emissions_tonnes,\n",
    "       merged_df2['import_trade_sum_usd'],merged_df2['import_weight_sum_kg'],merged_df2['import_quantity_sum'],\n",
    "       india_export_sum['export_trade_sum_usd'],india_export_sum['export_weight_sum_kg'],india_export_sum['export_quantity_sum']\n",
    "       ) \\\n",
    ".distinct()\n",
    "final_merged_spark = final_merged_spark.orderBy(final_merged_spark.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_emissions=sql_to_spark('global_emissions')\n",
    "spark_emissions = spark_emissions.withColumnRenamed(\"Entity\",\"country\")\n",
    "spark_emissions = spark_emissions.withColumnRenamed(\"Year\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_gdp=sql_to_spark('gdp_data')\n",
    "gdp_col = spark_gdp.columns\n",
    "gdp_col2 = gdp_col[2:]\n",
    "gdp_static_cols = gdp_col[:2]\n",
    "spark_gdp_2 = melt(spark_gdp, id_vars=gdp_static_cols, value_vars=gdp_col2)\n",
    "spark_gdp_2 = spark_gdp_2.withColumn(\"variable\",spark_gdp_2.variable.cast('int'))\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"variable\",\"year\")\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"value\",\"GDP\")\n",
    "spark_gdp_2 = spark_gdp_2.withColumnRenamed(\"Country \",\"country\")\n",
    "spark_gdp_2 = spark_gdp_2.select(['country','year','GDP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_export_df, india_import_df = sql_spark_trade(\"global_trade\",\"India\")\n",
    "india_export_df_min, india_export_df_max = min_int(india_export_df, 'year')\n",
    "india_import_df_min, india_import_df_max = min_int(india_import_df, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_gdp_spark = spark_gdp_2.filter(spark_gdp_2[\"country\"] == \"India\")\n",
    "india_gdp_spark_min, india_gdp_spark_max = min_int(india_gdp_spark, \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_emissions = spark_emissions.filter(spark_emissions[\"country\"] == \"India\")\n",
    "india_emissions_min, india_emissions_max = min_int(india_emissions, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_india_min, spark_india_max = min_int(spark_india, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_df = pd.DataFrame([{\"min\":india_export_df_min, \"max\":india_export_df_max},\n",
    " {\"min\":india_import_df_min, \"max\":india_import_df_max},\n",
    " {\"min\":india_gdp_spark_min, \"max\":india_gdp_spark_max},\n",
    " {\"min\":india_emissions_min, \"max\":india_emissions_max},\n",
    "#  {\"min\":spark_india_min,\"max\":spark_india_max}])\n",
    "                        ])\n",
    "lower_bound = years_df['min'].max()\n",
    "upper_bound = years_df['max'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_gdp_filt = india_gdp_spark.filter(india_gdp_spark[\"year\"] >= lower_bound)\n",
    "india_gdp_filt = india_gdp_filt.filter(india_gdp_filt[\"year\"] <= upper_bound)\n",
    "# india_gdp_filt = india_gdp_filt.orderBy(india_gdp_filt.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_emissions_filt = india_emissions.filter(india_emissions[\"year\"] >= lower_bound)\n",
    "india_emissions_filt = india_emissions_filt.filter(india_emissions_filt[\"year\"] <= upper_bound)\n",
    "# india_emissions_filt = india_emissions_filt.orderBy(india_emissions_filt.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_import_filt = india_import_df.filter(india_import_df[\"year\"] >= lower_bound)\n",
    "india_import_filt = india_import_filt.filter(india_import_filt[\"year\"] <= upper_bound)\n",
    "# india_import_filt = india_import_filt.orderBy(india_import_filt.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_export_filt = india_export_df.filter(india_export_df[\"year\"] >= lower_bound)\n",
    "india_export_filt = india_export_filt.filter(india_export_filt[\"year\"] <= upper_bound)\n",
    "# india_export_filt = india_export_filt.orderBy(india_export_filt.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = india_gdp_filt.join(india_emissions_filt, india_gdp_filt.year == india_emissions_filt.year, 'outer') \\\n",
    ".select(india_gdp_filt.country ,india_gdp_filt.year,india_gdp_filt.GDP, india_emissions_filt.annual_co2_emissions_tonnes) \\\n",
    ".distinct()\n",
    "\n",
    "merged_df = merged_df.orderBy(merged_df.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# india_import_sum = india_import_filt.groupBy('year').sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_import_sum = india_import_filt.groupBy('year').sum()\n",
    "india_import_sum = india_import_sum.orderBy(india_import_sum.year.asc())\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(trade_usd)\",\"import_trade_sum_usd\")\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(weight_kg)\",\"import_weight_sum_kg\")\n",
    "india_import_sum = india_import_sum.withColumnRenamed(\"sum(quantity)\",\"import_quantity_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_export_sum = india_export_filt.groupBy('year').sum()\n",
    "india_export_sum = india_export_sum.orderBy(india_export_sum.year.asc())\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(trade_usd)\",\"export_trade_sum_usd\")\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(weight_kg)\",\"export_weight_sum_kg\")\n",
    "india_export_sum = india_export_sum.withColumnRenamed(\"sum(quantity)\",\"export_quantity_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df.join(india_import_sum, merged_df.year == india_import_sum.year, 'outer') \\\n",
    ".select(merged_df.country ,merged_df.year,merged_df.GDP, merged_df.annual_co2_emissions_tonnes,\n",
    "       india_import_sum['import_trade_sum_usd'],india_import_sum['import_weight_sum_kg'],india_import_sum['import_quantity_sum']) \\\n",
    ".distinct()\n",
    "merged_df2 = merged_df2.orderBy(merged_df2.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_spark = merged_df2.join(india_export_sum, merged_df2.year == india_export_sum.year, 'outer') \\\n",
    ".select(merged_df2.country ,merged_df2.year,merged_df2.GDP, merged_df.annual_co2_emissions_tonnes,\n",
    "       merged_df2['import_trade_sum_usd'],merged_df2['import_weight_sum_kg'],merged_df2['import_quantity_sum'],\n",
    "       india_export_sum['export_trade_sum_usd'],india_export_sum['export_weight_sum_kg'],india_export_sum['export_quantity_sum']\n",
    "       ) \\\n",
    ".distinct()\n",
    "final_merged_spark = final_merged_spark.orderBy(final_merged_spark.year.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------+---------------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|country|year|GDP        |annual_co2_emissions_tonnes|import_trade_sum_usd|import_weight_sum_kg|import_quantity_sum|export_trade_sum_usd|export_weight_sum_kg|export_quantity_sum|\n",
      "+-------+----+-----------+---------------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|India  |1990|1236.671208|6.165790984E8              |37086019677         |3.2819250453E10     |3.6282271392E10    |27688482159         |3.8376259571E10     |4.480699398E10     |\n",
      "|India  |1991|1265.917481|6.567408929E8              |31062661051         |3.4226080286E10     |3.8234796507E10    |27430173100         |3.6816498359E10     |3.9646091225E10    |\n",
      "|India  |1992|1338.664595|6.987569497E8              |39084634969         |4.2689104735E10     |4.7599677372E10    |31470615068         |2.976200432E10      |3.3908169243E10    |\n",
      "|India  |1993|1407.473802|7.228942594E8              |37539123801         |4.5296021901E10     |5.0606677042E10    |33510556122         |3.6266524584E10     |4.0448555076E10    |\n",
      "|India  |1994|1503.745417|7.642419667E8              |44362429749         |4.8759351832E10     |5.3607525051E10    |38798524293         |3.4873621373E10     |3.7277200863E10    |\n",
      "|India  |1995|1620.175387|8.114607871E8              |56075516124         |5.0914090335E10     |5.5808446634E10    |47345419557         |4.6788286598E10     |4.9065093376E10    |\n",
      "|India  |1996|1741.084404|8.788269645E8              |61838156581         |6.7456906926E10     |7.3519563361E10    |49095594933         |3.575007322E10      |4.0058339268E10    |\n",
      "|India  |1997|1808.708364|9.15049313E8               |65244544106         |8.5926438534E10     |9.2643260702E10    |50999841765         |4.3681753702E10     |4.7468333112E10    |\n",
      "|India  |1998|1906.7799  |9.335492581E8              |68715993711         |1.09386297669E11    |1.22652887432E11   |49432478707         |3.5647035447E10     |3.9922565692E10    |\n",
      "|India  |1999|2067.740994|9.952330716E8              |82989264268         |2.094902297714E12   |2.105394003047E12  |54250796602         |2.6084693209E10     |3.152141065E10     |\n",
      "|India  |2000|2156.689919|1.029637759E9              |88809407353         |1.85812021693E11    |1.85863358242E11   |61829613736         |6.2039754428E10     |6.2374173447E10    |\n",
      "|India  |2001|2270.718563|1.035640714E9              |83082132139         |1.1884456079E11     |1.19019007281E11   |64777358825         |4.7158010606E10     |4.7277022275E10    |\n",
      "|India  |2002|2354.2651  |1.047595651E9              |93556550228         |1.29589372724E11    |1.29770545102E11   |74010134226         |1.06139036309E11    |1.0629646545E11    |\n",
      "|India  |2003|2544.112251|1.091873009E9              |117113653750        |1.91771233742E11    |1.91894740138E11   |87385242955         |8.4514295716E10     |8.4585830894E10    |\n",
      "|India  |2004|2774.420858|1.145545234E9              |160826958697        |1.49801280064E11    |1.49987863204E11   |112791341712        |1.01081742643E11    |1.01298270978E11   |\n",
      "|India  |2005|3039.128698|1.210145667E9              |228394163877        |1.78999653579E11    |1.78911566125E11   |150374185542        |1.45724897516E11    |1.45815844678E11   |\n",
      "|India  |2006|3331.592127|1.287149211E9              |290621658680        |2.00495797226E11    |1.9881307939E11    |181803936258        |1.56714305329E11    |1.56853552996E11   |\n",
      "|India  |2007|3628.001338|1.390253491E9              |351174204585        |2.22579072914E11    |2.22954969781E11   |222885270889        |1.55177229521E11    |1.55172601763E11   |\n",
      "|India  |2008|3757.289184|1.547991039E9              |528157375162        |2.48347446376E11    |2.4861701084E11    |280379347198        |1.45606483604E11    |1.45219274186E11   |\n",
      "|India  |2009|4026.380833|1.72012056E9               |432232768985        |2.80845050648E11    |2.8138454056E11    |277972997029        |1.74540231E11       |1.84777681356E11   |\n",
      "+-------+----+-----------+---------------------------+--------------------+--------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_merged_spark.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
